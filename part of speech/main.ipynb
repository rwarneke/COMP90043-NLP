{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging Mini Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treebank data sourced from [https://www.kaggle.com/nltkdata/penn-tree-bank](https://www.kaggle.com/nltkdata/penn-tree-bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from abc import ABC, abstractclassmethod\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Treebank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tagged_chunk(chunk):\n",
    "    \"\"\"\n",
    "    Input: raw text from a .pos file from the Penn Treebank Project dataset.\n",
    "    Output: A list of sentences, which are lists of (word, tag) pairs.\n",
    "    \"\"\"\n",
    "    lines = chunk.split(\"\\n\")\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    for line in lines:\n",
    "        if line == \"\":\n",
    "            if current_sentence:\n",
    "                # we have reached the end of the current sentence\n",
    "                sentences.append(current_sentence)\n",
    "                # start a new one\n",
    "                current_sentence = []\n",
    "            else:\n",
    "                # this is just boring whitespace, ignore it\n",
    "                continue\n",
    "        # strip off groupings\n",
    "        line = line.strip(\"[] \")\n",
    "        # split the line into word-tag pairs\n",
    "        word_tag_pairs = line.split()\n",
    "        for pair in word_tag_pairs:\n",
    "            # currently pair is in string form\n",
    "            # split it into word, tag\n",
    "            # sometimes word can unfortunately include the / symbol\n",
    "            # makes code messier\n",
    "            pieces = pair.split(\"/\")\n",
    "            tag = pieces[-1]\n",
    "            word = \"/\".join(pieces[:-1])\n",
    "            # not too bad!\n",
    "            pair = word, tag\n",
    "\n",
    "            # require letters with optional $ at the end\n",
    "            looks_like_a_real_tag = tag.strip(\"$\").isalpha()\n",
    "            if looks_like_a_real_tag:\n",
    "                current_sentence.append(pair)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are wrong, update is in fact required\n"
     ]
    }
   ],
   "source": [
    "# this code block will create the variable `chunks`,\n",
    "# a list of chunks, where a chunk is a list of sentences, where a sentence is\n",
    "# a list of (word, tag) pairs.\n",
    "\n",
    "# if `UPDATE_REQUIRED` is set to true, data from data/treebank is accessed and\n",
    "# parsed into chunks, and the result saved to data/parsed.json. Otherwise, the\n",
    "# chunks are read straight from data/parsed.json\n",
    "UPDATE_REQUIRED = False\n",
    "\n",
    "if not UPDATE_REQUIRED:\n",
    "    try:\n",
    "        with open(\"data/parsed.json\") as file:\n",
    "            chunks = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        # force update\n",
    "        print(\"You are wrong, update is in fact required\")\n",
    "        UPDATE_REQUIRED = True\n",
    "\n",
    "if UPDATE_REQUIRED:\n",
    "    def parse_all_tagged_chunks(path=\"data/treebank/tagged\"):\n",
    "        filenames = [x for x in os.listdir(path) if x.endswith(\".pos\")]\n",
    "        chunks = []\n",
    "        for fn in filenames:\n",
    "            fp = os.path.join(path, fn)\n",
    "            with open(fp) as file:\n",
    "                data = file.read()\n",
    "                parsed_chunk = parse_tagged_chunk(data)\n",
    "                chunks.append(parsed_chunk)\n",
    "        return chunks\n",
    "\n",
    "    chunks = parse_all_tagged_chunks()\n",
    "    with open(\"data/parsed.json\", \"w\") as file:\n",
    "        json.dump(chunks, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing parsed data for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intra-chunk sentences are related, but we aren't going to consider this context\n",
    "# instead, chunks will be summed (as lists) to produce datasets\n",
    "\n",
    "all_sentences = sum(chunks, [])\n",
    "\n",
    "def train_dev_test_split(data, dev_prop=0.1, test_prop=0.1):\n",
    "    n = len(data)\n",
    "    dev_size = int(n * dev_prop)\n",
    "    test_size = int(n * test_prop)\n",
    "    train_size = n - (dev_size + test_size)\n",
    "    return data[:train_size], data[train_size:train_size+dev_size], data[-test_size:]\n",
    "\n",
    "data_train, data_dev, data_test = train_dev_test_split(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagger(ABC):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractclassmethod\n",
    "    def train(self, sentences):\n",
    "        pass\n",
    "\n",
    "    @abstractclassmethod\n",
    "    def tag(self, sentence):\n",
    "        pass\n",
    "\n",
    "def test_tagger(tagger, test_data=data_dev, num_failures_to_print=0):\n",
    "    correct, n = 0, 0\n",
    "    failures_printed = 0\n",
    "    for sentence in test_data:\n",
    "        sentence_tags_hidden = [word for word, _ in sentence]\n",
    "        tagged = tagger.tag(sentence_tags_hidden)\n",
    "        mistake_made = False\n",
    "        for (_, tag), (_, true_tag) in zip(tagged, sentence):\n",
    "            n += 1\n",
    "            if tag == true_tag:\n",
    "                correct += 1\n",
    "            else:\n",
    "                mistake_made = True\n",
    "        if failures_printed < num_failures_to_print and mistake_made:\n",
    "            longest_word_len = max(len(word) for word, _ in sentence)\n",
    "            print(\"----- Example error ----\")\n",
    "            print(\"WORD\".rjust(longest_word_len), \"TRUE\".rjust(5), \"PRED\".rjust(5))\n",
    "            for (word, tag), (_, true_tag) in zip(tagged, sentence):\n",
    "                print(word.rjust(longest_word_len), true_tag.rjust(5), (tag if tag != true_tag else \"-\").rjust(5))\n",
    "            failures_printed += 1\n",
    "            print()\n",
    "    return correct / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramTagger(Tagger):\n",
    "    \"\"\"\n",
    "    Tag words with tag they appeared with most often in training data.\n",
    "\n",
    "    If word has not been seen before, tag it with the most common tag.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.trained = False\n",
    "\n",
    "    def train(self, sentences):\n",
    "        # we don't care about sentence context, so just combine into a big ol mash\n",
    "        tagged_words = sum(sentences, [])\n",
    "        tag_frequency = defaultdict(lambda: defaultdict(int))\n",
    "        overall_tag_counts = defaultdict(int)\n",
    "        for word, tag in tagged_words:\n",
    "            tag_frequency[word][tag] += 1\n",
    "            overall_tag_counts[tag] += 1\n",
    "        model = {}\n",
    "        for word, tag_freq_for_word in tag_frequency.items():\n",
    "            most_common_tag = max(tag_freq_for_word.keys(), key = tag_freq_for_word.get)\n",
    "            model[word] = most_common_tag\n",
    "        self.model = model\n",
    "        self.most_common_tag = max(overall_tag_counts, key = overall_tag_counts.get)\n",
    "        self.tag_frequency = tag_frequency\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        tagged = []\n",
    "        for word in sentence:\n",
    "            if word in self.model:\n",
    "                tag = self.model[word]\n",
    "            else:\n",
    "                tag = self.most_common_tag\n",
    "            tagged.append((word, tag))\n",
    "        return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = UnigramTagger()\n",
    "T.train(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline unigram tagger achieved  85.98% accuracy on development data ( 95.09% on training data)\n"
     ]
    }
   ],
   "source": [
    "acc = test_tagger(T, num_failures_to_print=0)\n",
    "train_acc = test_tagger(T, data_train)\n",
    "print(f\"Baseline unigram tagger achieved {100 * acc : .2f}% accuracy on development data ({100 * train_acc : .2f}% on training data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1989e97614739c78c0b52e1ed35be4323708ef6476b065f288fdb79d894ddac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
